---
title: "ST509 Computing Statistics Final"
author: "Hayoung Kim"
date: "6/19/2019"
output: 
  html_document:
    keep_md: true
---

## 1. (LUM)

```r
lum <- function(x, y, a=1, c=1, lambda=1, max.iter=200, eps =1.0e-7){
  # initialization
  y_0 <- y==1
  obj <- glm(y_0~x, family = binomial)
  alpha <- as.numeric(obj$coefficients [1])
  beta <- as.numeric(obj$coefficients [2])

  V_u <- function(u,c,a){
    obj <- ifelse(u<(c/(1+c)), 1-u, ((a/((1+c)*u-c+a))^a)/(1+c))
    return(obj)
  }
  
  for (iter in 1:max.iter) {
    
    f1<- function(x, y, alpha, beta, c, a) sum(V_u(y*(alpha+beta*x), c, a))
  
    new.alpha <- optimize(f1, c(alpha-2,alpha+2), tol=eps,
                          x=x, y=y, beta=beta, c=c, a=a)$minimum
    
    f2<- function(x, y, alpha, beta, c, a, lambda){
      mean(V_u(y*(alpha+beta*x),c,a))+(beta^2)*(lambda/2)
    }
    
    new.beta <- optimize(f2, c(beta-2,beta+2),tol=eps, x=x, y=y,
                         alpha=alpha, c=c, a=a, lambda=lambda)$minimum
    
    delta <- max(abs(c(new.beta - beta, new.alpha - alpha)))
    if (delta < eps) break
    beta <- new.beta
    alpha <- new.alpha
  }
  est <- c(alpha,beta)
  return(est)
}
```


## 2. (WSVM)
$$min_{\beta_0,\boldsymbol{\beta}}C\displaystyle\sum_{i=1}^{n} w_i\xi_i+\boldsymbol{\beta}^T\boldsymbol{\beta}$$
subject to $$y_i(\beta_0,\boldsymbol{\beta}^Tx_i)\geq1-\xi_i,\quad \xi_i\geq0,\quad \forall i=1,...,n$$
Lagrangian primal function of the linear weighted SVM is 
$$L_p:\boldsymbol{\beta}^T\boldsymbol{\beta}+C\displaystyle\sum_{i=1}^{n} w_i\xi_i+\displaystyle\sum_{i=1}^{n}\alpha_i\{1-\xi_i-y_i(\beta_0+\boldsymbol{\beta}^Tx_i)\}-\displaystyle\sum_{i=1}^{n}\gamma_i\xi_i$$
To solve this problem, 

1) take derivative with respect to primal variables $\beta_0,\boldsymbol{\beta},\xi_i$:
$$\frac{\partial}{\partial\boldsymbol{\beta}}L_p:\boldsymbol{\beta}=\frac{1}{2}\displaystyle\sum_{i=1}^{n}\alpha_iy_ix_i$$
$$\frac{\partial}{\partial\beta_0}L_p:\displaystyle\sum_{i=1}^{n}\alpha_iy_i=0$$
$$\frac{\partial}{\partial\xi_i}L_p:Cw_i-\alpha_i-\gamma_i=0$$
2) KKT complementary conditions:
$$\alpha_i\{1-\xi_i-y_i(\beta_0,\boldsymbol{\beta}^Tx_i)\}=0$$
$$\gamma_i\xi_i=0$$

Plugging these into $L_p$, these problem become dual problem and Quadratic problem.
$$L_p:\frac{1}{4}\displaystyle\sum_{i=1}^{n}\displaystyle\sum_{j=1}^{n} \alpha_i\alpha_jy_iy_jx_i^Tx_j+\displaystyle\sum_{i=1}^{n}\alpha_i-\displaystyle\sum_{i=1}^{n}\alpha_iy_i(\beta_0,\boldsymbol{\beta}^Tx_i)$$
$$L_p:\displaystyle\sum_{i=1}^{n}\alpha_i-\frac{1}{4}\displaystyle\sum_{i=1}^{n}\displaystyle\sum_{j=1}^{n} \alpha_i\alpha_jy_iy_jx_i^Tx_j$$
$$max_{\alpha_1,...,\alpha_n}\{\displaystyle\sum_{i=1}^{n}\alpha_i-\frac{1}{4}\displaystyle\sum_{i=1}^{n}\displaystyle\sum_{j=1}^{n} \alpha_i\alpha_jy_iy_jx_i^Tx_j\}$$
subject to $0\leq\alpha_i\leq Cw_i, \quad \displaystyle\sum_{i=1}^{n}\alpha_iy_i=0$

By solving above problem, we can get $\boldsymbol{\beta}$,
$$\boldsymbol{\beta}=\frac{1}{2}\displaystyle\sum_{i=1}^{n}\alpha_iy_ix_i$$

we can get $\beta_0$ by using support vector $k\in\{i:0<\alpha_i<Cw_i\}$
$$\alpha_k\neq 0 \implies 1-\xi_k-y_k(\beta_0,\boldsymbol{\beta}^Tx_k)=0$$
$$\alpha_k\neq Cw_k \implies \gamma_k \neq 0,\quad  \xi_k=0$$
$$ 1-y_k(\beta_0,\boldsymbol{\beta}^Tx_k)=0$$
$$ \frac{1}{y_k}-(\beta_0+\boldsymbol{\beta}^Tx_k)=0, \quad \frac{1}{y_k}=y_k$$
$$ \beta_0=y_k-\boldsymbol{\beta}^Tx_k$$


```r
wsvm <- function(x, y, pi=0.5, C=1){
  n<-nrow(x)
  p<-ncol(x)
  w <- y==-1
  w <- abs(w-pi)
  
  K <- x %*% t(x)
  K.star <- K * (outer(y, y)) + 1.0e-8 * diag(rep(1, n))
  d <- rep(1, n)
  A <- cbind(y,diag(n), -diag(n))
  b0 <- c(0,rep(0, n), -C*w)
  obj <- solve.QP(Dmat=K.star, dvec=d, Amat=A, bvec=b0, meq=1)
  alpha <- obj$solution # dual solution
  
  # indices for sv (no scale)
  sv.id <- which(1.0e-3 < alpha & alpha < (C*w - 1.0e-3))
  
  temp <- y[sv.id] - (t(y * K[, sv.id, drop = F]) %*% alpha)/2
  beta0 <- mean(temp) #little different so get mean
  beta <- (t(y * alpha) %*% x)/2
  # beta0 <- mean(y[sv.id] - x[sv.id,]%*%t(beta))
  
  est <- c(beta0, beta)
  return(est)
}
```

## 3. (EM algorithm for Right-censored Exponential Random Variable)

### (a)

$$logL(Y,\delta|\sigma)=-n_ulog\sigma-\frac{\displaystyle\sum_{i=1}^{n}Y_i}{\sigma}$$
$$\frac{\partial logL(Y,\delta|\sigma)}{\partial \sigma}=-\frac{n_u}{\sigma}+\frac{\displaystyle\sum_{i=1}^{n}Y_i}{\sigma^2}=0$$
$$\hat\sigma^{MLE} =\frac{\displaystyle\sum_{i=1}^{n}Y_i}{n_u}$$

### (b)

$$logL_C(\sigma|X)=\displaystyle\sum_{i=1}^{n}logf(X_i|\sigma)=-nlog\sigma-\frac{\displaystyle\sum_{i=1}^{n}X_i}{\sigma}$$
$$Q(\sigma,\sigma^{(\nu)},Y,\delta)=\mathbb{E}_{\sigma^{(\nu)}}\{logL_C(\sigma|X)|Y,\delta\}=-nlog\sigma-\frac{\displaystyle\sum_{i=1}^{n}\mathbb{E}_{\sigma^{(\nu)}}\{X_i|Y,\delta\}}{\sigma}$$
$$\mathbb{E}_{\sigma^{(\nu)}}\{logL_C(\sigma|X)|Y,\delta\}=\begin{cases}
    \mathbb{E}_{\sigma^{(\nu)}}\{X_i|Y,\delta=1\}=\mathbb{E}_{\sigma^{(\nu)}}\{X_i|X_i<R_i,Y_i=X_i\}=Y_i\\
    \mathbb{E}_{\sigma^{(\nu)}}\{X_i|Y,\delta=0\}=\mathbb{E}_{\sigma^{(\nu)}}\{X_i|X_i \geq R_i,Y_i=R_i\}=\sigma^{(\nu)}+R_i=\sigma^{(\nu)}+Y_i
  \end{cases}$$


$$Q(\sigma,\sigma^{(\nu)},Y,\delta)=-nlog\sigma-
\frac{\displaystyle\sum_{i=1}^{n_u}\mathbb{E}_{\sigma^{(\nu)}}\{X_i|Y_i,\delta_i=1\}+\displaystyle\sum_{i=n_u+1}^{n}\mathbb{E}_{\sigma^{(\nu)}}\{X_i|Y_i,\delta_i=0\}}{\sigma}$$
$$Q(\sigma,\sigma^{(\nu)},Y,\delta)=-nlog\sigma-
\frac{\displaystyle\sum_{i=1}^{n_u}Y_i+\displaystyle\sum_{i=n_u+1}^{n}(\sigma^{(\nu)}+Y_i)}{\sigma}$$
$$\frac{\partial Q(\sigma,\sigma^{(\nu)},Y,\delta)}{\partial\sigma}=-\frac{n}{\sigma}+
\frac{\displaystyle\sum_{i=1}^{n}Y_i+(n-n_u)\sigma^{(\nu)}}{\sigma^2}=0$$
$$\sigma^{(\nu+1)}= 
\frac{\displaystyle\sum_{i=1}^{n}Y_i+(n-n_u)\sigma^{(\nu)}}{n}$$

### (c)


```r
em <-function(y, delta, max.iter=100, eps =1.0e-5){
  # initialize
  temp.sigma <- 1
  
  #preprocessing
  y <- as.vector(y)
  delta <- as.vector(delta)
  
  n <- length(y)
  n_u <- length(delta[delta == 1])
  sum_y <- sum(y)
  
  for (i in 1:max.iter){
    new.sigma <- (sum_y+(n-n_u)*temp.sigma)
    if (max(abs(new.sigma - temp.sigma)) < eps) break
    sigma <- new.sigma
  }
  
  hat.sigma <- new.sigma
  return(hat.sigma)
}
```

## 4. (CI for Possion Regression)

### (a)

```r
mcmc <- function(x, y, n.sample=10000, step=0.3){
  x <- cbind(rep(1,n),x)
  n <- nrow(x)
  p <- ncol(x)
  
  step_list <- rep(step,p)
  post.beta <- matrix(0, n.sample, p)
  ac.ratio <- matrix(0, n.sample, p)
  
  prior.m <- 0
  prior.s <- 1000 # for vague prior
  
  #initialize
  post.beta[1,] <- beta <- rep(1, p)
  eta <- x %*% beta
  mu <- exp(eta)
  
  log.like  <- sum(y * log(mu) - mu)
  
  for (iter in 1:n.sample){
    beta.new <- beta
    # candidate
    for (j in 1:p){
      beta.new[j] <- beta[j] + rnorm(1, 0, step_list[j]) 
      eta.new <- x %*% beta.new
      mu.new  <- exp(eta.new)
      
      # prior
      log.prior     <- dnorm(beta[j],     prior.m, prior.s, log = T)
      log.prior.new <- dnorm(beta.new[j], prior.m, prior.s, log = T)
      
      # liklihood
      log.like.new <- sum(y * log(mu.new) - mu.new)
      
      # ratio
      temp <- exp((log.like.new + log.prior.new) - (log.like + log.prior))
      rho <- min(1, temp)
      
      if (runif(1) < rho) {
        ac.ratio[iter,j] <- 1
        beta[j] <- beta.new[j]
        log.like  <- log.like.new
        eta <- x %*% beta
        mu <- exp(eta)
      }
    }
    
    post.beta[iter,] <- beta
  }
  
  samples <- post.beta
  return(samples)
}
```

### (b)

```r
bayes.ci <- function(obj, alpha=0.05){
  n.sample <- nrow(obj)
  p <- ncol(obj)
  
  lower <- apply(obj, 2, quantile, probs = alpha/2)
  upper <- apply(obj, 2, quantile, probs = 1-alpha/2)
  
  return(cbind(lower,upper))
}
```


### (c)

```r
boot.ci <- function(x, y, B=500, alpha=0.05){
  beta.bt <- matrix(0, B, p+1)
  for (b in 1:B) {
    id.bt <- sample(n, replace = T)
    x.bt <- x[id.bt,]
    y.bt <- y[id.bt]
    obj<- glm(y.bt~x.bt, family = poisson)
    beta.bt[b,]<-obj$coefficients 
  }
  
  # percentile 
  lower <- apply(beta.bt, 2, quantile, probs = alpha/2)
  upper <- apply(beta.bt, 2, quantile, probs = 1-alpha/2)
  
  return(cbind(lower,upper))
}
```
